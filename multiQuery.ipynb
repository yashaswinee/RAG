{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7018adf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a tasty toast\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ['GEMINI_API_KEY'] = os.environ['GEMINI_API_KEY']\n",
    "\n",
    "os.environ['LANGSMITH_API_KEY']= os.environ['LANGSMITH_API_KEY']\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGSMITH_ENDPOINT']='https://api.smith.langchain.com'\n",
    "os.environ['LANGSMITH_PROJECT']='RAG-fusion'\n",
    "\n",
    "from langsmith import traceable\n",
    "print(os.environ['TASTY_TOAST'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1a05e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('smallContent.txt', 'r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc0c9779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='He was too much absorbed with his own thoughts to give any immediate\n",
      "     answer to my remonstrance. He leaned upon his hand, with his untasted\n",
      "     breakfast before him, and he stared at the slip of paper which he had\n",
      "     just drawn from its envelope. Then he took the envelope itself, held\n",
      "     it up to the light, and very carefully studied both the exterior and\n",
      "     the flap.'\n"
     ]
    }
   ],
   "source": [
    "# splits\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap=10)\n",
    "splits = text_splitter.create_documents([data])\n",
    "print(splits[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8df6723f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_49161/3068404039.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "2025-11-03 11:05:29.760190: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-03 11:05:30.305380: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-03 11:05:30.305420: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-03 11:05:30.309108: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-03 11:05:30.648422: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-03 11:05:32.241977: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# embed\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = 'intfloat/e5-large'\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a07b44a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    google_api_key=os.environ[\"GEMINI_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cc0cf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='You are an AI language model assistant. Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines. Original question: {question}'), additional_kwargs={})]) middle=[ChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), temperature=0.0, max_retries=2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7763f4701e40>, default_metadata=(), model_kwargs={}), StrOutputParser()] last=RunnableLambda(...)\n"
     ]
    }
   ],
   "source": [
    "# template\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    " \n",
    "# gives 6 questions based on user input\n",
    "@traceable \n",
    "def gen_queries():\n",
    "    generate_queries = (\n",
    "        prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        | (lambda x : x.split(\"\\n\"))\n",
    "    )\n",
    "    return generate_queries\n",
    "\n",
    "generate_queries = gen_queries()\n",
    "print(generate_queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91aed79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "persist_directory = './chroma_e5_db'\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,  \n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "891a2bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "['{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"page_content\": \"\\\\\"It is Porlock\\'s writing,\\\\\" said he thoughtfully. \\\\\"I can hardly doubt\\\\n     that it is Porlock\\'s writing, though I have seen it only twice\\\\n     before. The Greek e with the peculiar top flourish is distinctive.\\\\n     But if it is Porlock, then it must be something of the very first\\\\n     importance.\\\\\"\\\\n\\\\n     He was speaking to himself rather than to me; but my vexation\\\\n     disappeared in the interest which the words awakened.\\\\n\\\\n     \\\\\"Who then is Porlock?\\\\\" I asked.\", \"type\": \"Document\"}}', '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"page_content\": \"He was too much absorbed with his own thoughts to give any immediate\\\\n     answer to my remonstrance. He leaned upon his hand, with his untasted\\\\n     breakfast before him, and he stared at the slip of paper which he had\\\\n     just drawn from its envelope. Then he took the envelope itself, held\\\\n     it up to the light, and very carefully studied both the exterior and\\\\n     the flap.\", \"type\": \"Document\"}}', '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"page_content\": \"shark, the jackal with the lion--anything that is insignificant in\\\\n     companionship with what is formidable: not only formidable, Watson,\\\\n     but sinister--in the highest degree sinister. That is where he comes\\\\n     within my purview. You have heard me speak of Professor Moriarty?\\\\\"\", \"type\": \"Document\"}}', '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"page_content\": \"mathematics that it is said that there was no man in the scientific\\\\n     press capable of criticizing it? Is this a man to traduce?\\\\n     Foul-mouthed doctor and slandered professor--such would be your\\\\n     respective roles! That\\'s genius, Watson. But if I am spared by lesser\\\\n     men, our day will surely come.\\\\\"\", \"type\": \"Document\"}}']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_232471/3774749383.py:8: LangChainBetaWarning: The function `load` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [load(docs) for docs in unique_docs]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.load import dumps, load\n",
    "\n",
    "# parallel process\n",
    "def get_unique_union(documents: list[list]):\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [load(docs) for docs in unique_docs]\n",
    "\n",
    "question = \"I don't understand the context. Give me summary.\"\n",
    "\n",
    "@traceable\n",
    "def get_docs_retrieval_chain():\n",
    "    retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "\n",
    "    docs = retrieval_chain.invoke({\"question\": question})\n",
    "\n",
    "    print(len(docs))\n",
    "    print(docs)\n",
    "    return retrieval_chain\n",
    "\n",
    "retrieval_chain = get_docs_retrieval_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d67a943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context describes a character (likely Sherlock Holmes) examining a mysterious, important message from someone named Porlock, whose distinctive writing he recognizes. He is deeply absorbed in thought about the message. The conversation then shifts to the formidable and sinister Professor Moriarty, a mathematical genius, suggesting a connection between Porlock's message and Moriarty.\n"
     ]
    }
   ],
   "source": [
    "# Final RAG\n",
    "from operator import itemgetter\n",
    "\n",
    "@traceable\n",
    "def final_query():\n",
    "    template = \"\"\"Your job is to give a concise answer. Answer the question based on the context:\n",
    "    {context},\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    final_rag_chain = (\n",
    "        {\"context\": retrieval_chain,\n",
    "        \"question\": itemgetter(\"question\")}\n",
    "        | prompt \n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    response = final_rag_chain.invoke({\"question\": question})\n",
    "    return response\n",
    "\n",
    "response = final_query()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42783e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
